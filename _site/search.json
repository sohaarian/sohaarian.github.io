[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Group Project",
    "section": "",
    "text": "Soha Arian"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Soha Arian.\nI am a graduate student interested in cybersecurity, technology, and policy."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/j2/fnf5s_0x3rv7xfjztkrrrgp80000gn/T//RtmpaMOpdw/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab2.html#indexing-data-using",
    "href": "lab2.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "lab2.html#loading-data-from-github-remote",
    "href": "lab2.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "lab2.html#load-data-from-islr-website",
    "href": "lab2.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "lab2.html#additional-graphical-and-numerical-summaries",
    "href": "lab2.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "lab2.html#linear-regression",
    "href": "lab2.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/j2/fnf5s_0x3rv7xfjztkrrrgp80000gn/T//RtmpaMOpdw/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "lab2.html#multiple-linear-regression",
    "href": "lab2.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "lab2.html#non-linear-transformations-of-the-predictors",
    "href": "lab2.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab2.html#qualitative-predictors",
    "href": "lab2.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "lab2.html#interaction-terms-including-interaction-and-single-effects",
    "href": "lab2.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9952758\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "lab1.html#create-object-using-the-assignment-operator--",
    "href": "lab1.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "lab1.html#using-function",
    "href": "lab1.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "lab1.html#using---operators",
    "href": "lab1.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "lab1.html#matrix-operations",
    "href": "lab1.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9952758\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "lab1.html#simple-descriptive-statistics-base",
    "href": "lab1.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "lab1.html#visualization-using-r-graphics-without-packages",
    "href": "lab1.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Soha Arian’s Research",
    "section": "",
    "text": "Soha Arian"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "I reviewed Chapters 3 to 7 in R4DS, Section 3.5 in Data Programming. Using this information, I will run an exploratory data analysis with R using the TEDS2016 dataset.\n\n\nThis code downloads the TEDS2016 file in Stata format and loads it into R.\n\nlibrary(haven)\nlibrary(ggplot2)\n\nTEDS_2016 &lt;- haven::read_dta(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nI noticed that some variables are stored as coded numbers instead of clear words, which makes them harder to interpret until labels are added. I also found missing values in multiple columns, which can change results if they are not handled. Finally, some answers are non substantive, such as no response.\n\n\n\nMissing values can be handled by removing rows that have missing values for the variables being analyzed or by replacing missing numeric values with a reasonable value such as the mean. For this assignment, I remove rows where Tondu is missing because Tondu is required for the next steps. I also replace missing age values with the average age so age can still be used.\n\n\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\n\n\n\n\n\nTEDS_clean &lt;- TEDS_2016[, c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")]\nTEDS_clean &lt;- na.omit(TEDS_clean)\n\n\n\n\n\nTo explore Tondu with categorical variables such as female, DPP, Taiwanese, and Econ_worse, I use cross tabulations and proportions to compare group patterns. To explore Tondu with numeric or ordered variables such as age, income, and education, I compare averages by Tondu category. These methods provide a clear first look at differences across groups.\n\n\n\n\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_wrap(~ female)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_grid(female ~ DPP)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe votetsai variable shows whether a respondent voted for Tsai Ing wen. I first check the overall counts for votetsai, then compare it with Tondu using a cross tabulation to see how voting relates to Tondu attitudes.\n\n\n\nnames(TEDS_2016)\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\n\n\n\n\n\n\n\nTEDS_2016$Tondu &lt;- as.numeric(\n  TEDS_2016$Tondu,\n  labels = c(\n    \"Unification now\",\n    \"Status quo, unif. in future\",\n    \"Status quo, decide later\",\n    \"Status quo forever\",\n    \"Status quo, indep. in future\",\n    \"Independence now\",\n    \"No response\"\n  )\n)\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\")\n\n\n\n\n\n\n\n\n\nlibrary(haven)\nlibrary(ggplot2)\n\nTEDS_2016 &lt;- haven::read_dta(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nTEDS_clean &lt;- TEDS_2016[, c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\",\"votetsai\")]\nTEDS_clean &lt;- na.omit(TEDS_clean)\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_wrap(~ votetsai)"
  },
  {
    "objectID": "assignment1.html#import-the-dataset",
    "href": "assignment1.html#import-the-dataset",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "This code downloads the TEDS2016 file in Stata format and loads it into R.\n\nlibrary(haven)\nlibrary(ggplot2)\n\nTEDS_2016 &lt;- haven::read_dta(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")"
  },
  {
    "objectID": "assignment1.html#problems-i-see-in-the-dataset",
    "href": "assignment1.html#problems-i-see-in-the-dataset",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "I noticed that some variables are stored as coded numbers instead of clear words, which makes them harder to interpret until labels are added. I also found missing values in multiple columns, which can change results if they are not handled. Finally, some answers are non substantive, such as no response."
  },
  {
    "objectID": "assignment1.html#dealing-with-missing-values",
    "href": "assignment1.html#dealing-with-missing-values",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "Missing values can be handled by removing rows that have missing values for the variables being analyzed or by replacing missing numeric values with a reasonable value such as the mean. For this assignment, I remove rows where Tondu is missing because Tondu is required for the next steps. I also replace missing age values with the average age so age can still be used.\n\n\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\n\n\n\n\n\nTEDS_clean &lt;- TEDS_2016[, c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")]\nTEDS_clean &lt;- na.omit(TEDS_clean)"
  },
  {
    "objectID": "assignment1.html#relationship-between-tondu-and-other-variables",
    "href": "assignment1.html#relationship-between-tondu-and-other-variables",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "To explore Tondu with categorical variables such as female, DPP, Taiwanese, and Econ_worse, I use cross tabulations and proportions to compare group patterns. To explore Tondu with numeric or ordered variables such as age, income, and education, I compare averages by Tondu category. These methods provide a clear first look at differences across groups.\n\n\n\n\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_wrap(~ female)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_grid(female ~ DPP)"
  },
  {
    "objectID": "assignment1.html#looking-at-votetsai",
    "href": "assignment1.html#looking-at-votetsai",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "The votetsai variable shows whether a respondent voted for Tsai Ing wen. I first check the overall counts for votetsai, then compare it with Tondu using a cross tabulation to see how voting relates to Tondu attitudes.\n\n\n\nnames(TEDS_2016)\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\""
  },
  {
    "objectID": "assignment1.html#frequency-table-and-bar-chart-for-tondu",
    "href": "assignment1.html#frequency-table-and-bar-chart-for-tondu",
    "title": "Assignment 1 – Brush up R and Quarto",
    "section": "",
    "text": "TEDS_2016$Tondu &lt;- as.numeric(\n  TEDS_2016$Tondu,\n  labels = c(\n    \"Unification now\",\n    \"Status quo, unif. in future\",\n    \"Status quo, decide later\",\n    \"Status quo forever\",\n    \"Status quo, indep. in future\",\n    \"Independence now\",\n    \"No response\"\n  )\n)\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\")\n\n\n\n\n\n\n\n\n\nlibrary(haven)\nlibrary(ggplot2)\n\nTEDS_2016 &lt;- haven::read_dta(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nTEDS_clean &lt;- TEDS_2016[, c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\",\"votetsai\")]\nTEDS_clean &lt;- na.omit(TEDS_clean)\n\nggplot(TEDS_clean, aes(x = Tondu)) +\n  geom_bar(fill = \"deeppink\") +\n  facet_wrap(~ votetsai)"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "assignment3",
    "section": "",
    "text": "Soha Arian"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "",
    "text": "This assignment shows how different AI models respond to the same research task. The goal is to design prompts, evaluate model outputs, refine instructions, and understand how prompt structure impacts response quality."
  },
  {
    "objectID": "assignment2.html#objective",
    "href": "assignment2.html#objective",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "",
    "text": "This assignment shows how different AI models respond to the same research task. The goal is to design prompts, evaluate model outputs, refine instructions, and understand how prompt structure impacts response quality."
  },
  {
    "objectID": "assignment2.html#design-prompt",
    "href": "assignment2.html#design-prompt",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Design Prompt",
    "text": "Design Prompt\nFor the first test, I submitted the following prompt to multiple AI models:\nWrite a 1,500 word literature review that reads like an analytical discussion rather than a textbook summary. Start by explaining the purpose and scope of the review. Include a methodology section describing how ideas, themes, and findings are compared across studies. Add key insights by identifying patterns, similarities, and differences. Highlight important trends, challenges, and meaningful research gaps. Finish by suggesting one realistic and testable hypothesis that logically follows from the discussion. Maintain a formal but clear writing style, prioritizing explanation and reasoning over definitions or lists.\nThe prompt was submitted to ChatGPT, Copilot, and Grok 3."
  },
  {
    "objectID": "assignment2.html#model-response-analysis",
    "href": "assignment2.html#model-response-analysis",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Model Response Analysis",
    "text": "Model Response Analysis\nEach model response was evaluated using the following criteria:\nStructure\nDid it include a methodology section and follow a systematic review format?\nSynthesis\nWere key findings from data mining and machine learning applications well-summarized?\nTrends and Gaps\nDid it identify meaningful trends and research gaps?\nHypothesis Quality\nWas the proposed hypothesis testable and relevant?\nReferences\nAre the citations accurate (check using Google Scholar or Semantic Scholar)"
  },
  {
    "objectID": "assignment2.html#analyze-model-responses",
    "href": "assignment2.html#analyze-model-responses",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Analyze Model Responses",
    "text": "Analyze Model Responses\nStructure\nRegarding the structure, ChatGPT did a great job because it included a specific section explaining its methodology and followed a professional format. Copilot was also very organized and used clear headings that made the report easy to navigate. Grok 3 included a methodology paragraph as well, but the overall response felt more like one long essay rather than a structured review with clear sections.\nSynthesis\nWhen it comes to summarizing data mining and machine learning, ChatGPT was the only model that successfully stayed on topic. It explained how researchers are trying to balance making models accurate while also making them easy for humans to understand. Copilot completely missed the mark here because it wrote about technology in classrooms instead of data mining. Grok 3 focused almost entirely on how AI affects jobs and the economy, which was interesting but didn’t cover enough of the technical machine learning details.\nTrends and Gaps\nAll three models tried to identify trends, but ChatGPT was the most relevant to the prompt by pointing out that researchers are moving away from simple predictions and looking for deeper causes. Copilot identified gaps in how technology affects different students, which was good but for the wrong subject. Grok 3 identified a very modern trend regarding “agentic AI” and noted that we still don’t have enough long-term data on how these systems behave.\nHypothesis Quality\nThe hypotheses provided were a bit of a mixed bag. ChatGPT created a strong, testable guess about whether adaptive models work better than static ones over a long period of time. Copilot also wrote a clear hypothesis about student learning, but it wasn’t relevant to the actual assignment topic of data mining. Grok 3 failed this part entirely because the response cut off at the end, so it never actually provided a hypothesis to test.\nReferences\nFor the references and citations, all of the models struggled. None of them provided a list of specific papers, authors, or links that could be double-checked on Google Scholar or Semantic Scholar. They all spoke about “the literature” in a general way without giving credit to specific researchers, which makes it hard to verify if the information is coming from a real study.\nStrengths and Weaknesses\nIn summary, ChatGPT’s main strength was following the instructions perfectly and staying on the right topic, but its weakness was the lack of real citations. Copilot’s strength was its very clean and easy-to-read structure, but its major weakness was hallucinating the wrong topic for the review. Grok 3’s strength was providing very deep and unique thoughts about current AI trends, but its biggest weakness was being incomplete and cutting off before finishing the task."
  },
  {
    "objectID": "assignment2.html#refined-prompts",
    "href": "assignment2.html#refined-prompts",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Refined Prompts",
    "text": "Refined Prompts\nAfter reviewing weaknesses, I revised the prompts.\nRefined Prompt for ChatGPT\nThe biggest problem with ChatGPT’s first answer was that it did not name any real scientists or papers. It spoke in a very general way, which makes it hard to prove the information is true. This new prompt tells ChatGPT that it must include specific names and dates for its sources. It also asks for a bibliography at the end so we can check the work using Google Scholar or Semantic Scholar to make sure the citations are accurate.\nNew Prompt for ChatGPT:\nWrite a 1,500-word literature review about data mining and machine learning. You must include real in-text citations from academic studies, such as (Author, Year), to support your points. Include a methodology section that explains how you compared different studies. At the very end, provide a full list of references and one realistic, testable hypothesis that researchers could use for a new study.\nRefined Prompt for Microsoft Copilot\nMicrosoft Copilot failed the first time because it wrote about the wrong topic, focusing on education instead of computer science. To fix this, the new prompt uses very strict instructions to stay on the subject of data mining and machine learning. Since Copilot is good at making things look neat, the prompt asks for a clear structure with specific sections for trends and gaps. This will help the model stay focused on the technical requirements of the assignment.\nNew Prompt for Copilot:\nWrite an analytical literature review of exactly 1,500 words about the technical applications of Data Mining and Machine Learning. Do not write about “human learning” or “education”; you must focus only on computer science topics like algorithm performance and data patterns. Organize the review with clear sections for methodology, key findings, and research gaps. End the discussion with a formal, testable hypothesis.\nRefined Prompt for Grok 3\nGrok 3’s first response was smart, but it was too short and it cut off before it could finish the job. It also focused too much on jobs and money instead of the actual technology. The new prompt tells Grok 3 to act like a data scientist and write a much longer review. It also tells the model to balance its time better so that it does not stop talking before it gets to the hypothesis at the very end.\nNew Prompt for Grok 3:\nImagine you are a data scientist writing a 2,000-word systematic literature review on data mining and machine learning in fields like healthcare and finance. You must provide a clear methodology and focus on the technical side of how models work, such as their accuracy and how they handle complex data. Ensure you manage the length of your response so it is complete. You must finish with a bold and testable hypothesis and a final conclusion."
  },
  {
    "objectID": "assignment2.html#final-prompt",
    "href": "assignment2.html#final-prompt",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Final Prompt",
    "text": "Final Prompt\nAfter testing the new prompts, the results were much better because each AI finally followed the specific rules it missed the first time. ChatGPT became more trustworthy by adding real names and dates of scientists to support its ideas, while Copilot finally stopped talking about the wrong topic and stayed focused only on computer science. Grok 3 also did a better job by finishing its entire report and providing a smart “science guess” or hypothesis at the end instead of cutting off early. This shows that when you give an AI very clear instructions and fix its past mistakes, it can write a much more professional and complete paper for your project.\nAfter collecting revised outputs, I used this synthesis prompt:\nI am giving you three different drafts for a literature review on Data Mining and Machine Learning. Please combine them into one final 2,000-word systematic review. Use the real-world citations and the methodology from the first draft, the clear headings and organization from the second draft, and the modern technical insights from the third draft. Make sure the final document flows smoothly, maintains a formal tone, and ends with the most realistic and testable hypothesis. [I insert the three model outputs here.]"
  },
  {
    "objectID": "assignment2.html#reflection",
    "href": "assignment2.html#reflection",
    "title": "Assignment 2 – Prompt Exercise",
    "section": "Reflection",
    "text": "Reflection\nHow did the models respond differently?\nEach model handled the systematic review in a different way. ChatGPT focused on the technical history and professional structure of data mining, making sure to connect old methods with new ones. Microsoft Copilot focused more on the layout and organization of the report, though it struggled to stay on the correct topic at the beginning. Grok 3 focused on the newest trends and how AI affects the real world right now, but it had a hard time finishing long responses and often stopped before the end.\nWhich refinements worked best?\nThe prompt refinements that worked best were the ones that added very specific requirements. For ChatGPT, telling it to include real-world citations with authors and dates made the review much more accurate. For Copilot, the most helpful change was using bold instructions to stay strictly on the topic of computer science so it wouldn’t talk about other subjects. For Grok 3, the best fix was asking it to be more concise and watch its length, which helped it complete the final sections and the hypothesis.\nWhat did this exercise demonstrate?\nI learned that using AI for academic reviews requires a lot of back-and-forth work to get a good result. You cannot just use the first answer the AI gives you; you have to check for missing parts and fix the instructions to get better details. I also learned that it is best to use multiple models because one might be better at organizing while another is better at finding modern trends. The most important lesson is that a human must guide the AI and verify all the facts to make sure the final paper is correct and complete."
  },
  {
    "objectID": "week1-reflection.html#ai-and-originality",
    "href": "week1-reflection.html#ai-and-originality",
    "title": "Weekly Reflections",
    "section": "AI and originality",
    "text": "AI and originality\nThis week, I learned how AI can enhance learning without replacing original thought. AI is beneficial for me when used as a support tool for understanding concepts, while originality comes from how ideas are interpreted and communicated by me. I use AI to help me study! For example,it is beneficial for me to break down difficult concepts so that I can understand them in more simple terms and real-world scenario examples."
  },
  {
    "objectID": "week1-reflection.html#what-is-agi-how-does-agi-affect-scientific-research",
    "href": "week1-reflection.html#what-is-agi-how-does-agi-affect-scientific-research",
    "title": "Weekly Reflections",
    "section": "What is AGI? How does AGI affect scientific research?",
    "text": "What is AGI? How does AGI affect scientific research?\nI learned that Artificial General Intelligence (AGI) refers to systems capable of performing human level tasks. AGI could help enhance scientific research through faster data analysis, but it also causes ethical concerns in regards to responsible use."
  },
  {
    "objectID": "week1-reflection.html#week-2-reflection",
    "href": "week1-reflection.html#week-2-reflection",
    "title": "Weekly Reflections",
    "section": "WEEK 2 REFLECTION",
    "text": "WEEK 2 REFLECTION"
  },
  {
    "objectID": "week1-reflection.html#how-to-start-using-ai-for-your-project",
    "href": "week1-reflection.html#how-to-start-using-ai-for-your-project",
    "title": "Weekly Reflections",
    "section": "How to start using AI for your project?",
    "text": "How to start using AI for your project?\nI believe I can use AI in my project by helping me turn our STATA data zip file to help us use it in R studio. AI can help us let the data be read in R studio."
  },
  {
    "objectID": "week1-reflection.html#which-two-ai-models-are-you-using-how-can-you-leverage-them-to-help-your-research",
    "href": "week1-reflection.html#which-two-ai-models-are-you-using-how-can-you-leverage-them-to-help-your-research",
    "title": "Weekly Reflections",
    "section": "Which two AI models are you using? How can you leverage them to help your research?",
    "text": "Which two AI models are you using? How can you leverage them to help your research?\nI am using CHATGPT and Google Gemini for my project. I am new to using R, so I will ask it to help me if I run into any errors uploading the STATA zip file to R studio."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Soha Arian",
    "section": "",
    "text": "Hello! This website will show my coursework, projects, and notes."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume 2026",
    "section": "",
    "text": "Irving, TX 75063\n(214) 235-2006\nsohaarian1802@gmail.com\nLinkedIn\n\n\n\nThe University of Texas at Dallas\nB.S. Quantitative Finance (GPA: 3.5)\nAugust 2023 – December 2025\nConcentration: Risk Management and Cybersecurity\nM.S. Cybersecurity, Technology & Policy\nJanuary 2026 – May 2027\n\n\n\n\nBusiness Lines Underwriting Intern\nLiberty Mutual Insurance | May 2025 – August 2025\n\nEnsured compliance with training deadlines and retention requirements\n\nMonitored certificates of insurance and digital documentation for policy renewals\n\nReviewed underwriting documentation to identify errors and manage risk exposure\n\nPresident, Gamma Iota Sigma – Beta Phi Chapter\nUniversity of Texas at Dallas | April 2024 – January 2025\n\nLed initiatives connecting students with professionals in risk management and insurance\n\nRepresented UT Dallas at RiskWorld 2025 and the Houston Marine & Energy Conference\n\nHelped organize an RMI Career Fair supporting students pursuing insurance careers\n\nInvestor Relations Intern\nLionchase North America | May 2024 – August 2024\n\nOrganized investment agreements and managed deadlines and renewals\n\nCollaborated with legal teams to resolve contract issues\n\nUpdated CRM systems and internal communication logs\n\nLegal Intern\nOpening Doors | August 2023 – May 2024\n\nOrganized sensitive legal documents in compliance with retention guidelines\n\nReviewed supporting documentation for immigration cases\n\nCommunicated professionally and confidentially with clients\n\n\n\n\n\nGoogle & IBM Project Management\n\nDeveloped risk registers and project timelines, improving team efficiency by 20%\n\nGoogle Cybersecurity\n\nIdentified cybersecurity risks relevant to claims investigation and risk assessment\n\n\n\n\n\nRelevant Coursework\nPrinciples of Risk Management & Insurance Commercial Property Risk Management Enterprise Risk Management Operational Risk Management Cybersecurity Risk Management\nBusiness Liability Risk Management and Insurance\nFinance & Risk Management\nFinancial Analysis, Risk Assessment, Claims Evaluation\nTechnical Skills\nPython, Java, Tableau, SIEM Tools, CRM Systems"
  },
  {
    "objectID": "cv.html#academic-experience",
    "href": "cv.html#academic-experience",
    "title": "Resume 2026",
    "section": "",
    "text": "The University of Texas at Dallas\nB.S. Quantitative Finance (GPA: 3.5)\nAugust 2023 – December 2025\nConcentration: Risk Management and Cybersecurity\nM.S. Cybersecurity, Technology & Policy\nJanuary 2026 – May 2027"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Resume 2026",
    "section": "",
    "text": "Business Lines Underwriting Intern\nLiberty Mutual Insurance | May 2025 – August 2025\n\nEnsured compliance with training deadlines and retention requirements\n\nMonitored certificates of insurance and digital documentation for policy renewals\n\nReviewed underwriting documentation to identify errors and manage risk exposure\n\nPresident, Gamma Iota Sigma – Beta Phi Chapter\nUniversity of Texas at Dallas | April 2024 – January 2025\n\nLed initiatives connecting students with professionals in risk management and insurance\n\nRepresented UT Dallas at RiskWorld 2025 and the Houston Marine & Energy Conference\n\nHelped organize an RMI Career Fair supporting students pursuing insurance careers\n\nInvestor Relations Intern\nLionchase North America | May 2024 – August 2024\n\nOrganized investment agreements and managed deadlines and renewals\n\nCollaborated with legal teams to resolve contract issues\n\nUpdated CRM systems and internal communication logs\n\nLegal Intern\nOpening Doors | August 2023 – May 2024\n\nOrganized sensitive legal documents in compliance with retention guidelines\n\nReviewed supporting documentation for immigration cases\n\nCommunicated professionally and confidentially with clients"
  },
  {
    "objectID": "cv.html#certifications",
    "href": "cv.html#certifications",
    "title": "Resume 2026",
    "section": "",
    "text": "Google & IBM Project Management\n\nDeveloped risk registers and project timelines, improving team efficiency by 20%\n\nGoogle Cybersecurity\n\nIdentified cybersecurity risks relevant to claims investigation and risk assessment"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Resume 2026",
    "section": "",
    "text": "Relevant Coursework\nPrinciples of Risk Management & Insurance Commercial Property Risk Management Enterprise Risk Management Operational Risk Management Cybersecurity Risk Management\nBusiness Liability Risk Management and Insurance\nFinance & Risk Management\nFinancial Analysis, Risk Assessment, Claims Evaluation\nTechnical Skills\nPython, Java, Tableau, SIEM Tools, CRM Systems"
  },
  {
    "objectID": "weekly-reflections.html#ai-and-originality",
    "href": "weekly-reflections.html#ai-and-originality",
    "title": "Weekly Reflections",
    "section": "AI and originality",
    "text": "AI and originality\nThis week, I learned how AI can enhance learning without replacing original thought. AI is beneficial for me when used as a support tool for understanding concepts, while originality comes from how ideas are interpreted and communicated by me. I use AI to help me study! For example,it is beneficial for me to break down difficult concepts so that I can understand them in more simple terms and real-world scenario examples."
  },
  {
    "objectID": "weekly-reflections.html#what-is-agi-how-does-agi-affect-scientific-research",
    "href": "weekly-reflections.html#what-is-agi-how-does-agi-affect-scientific-research",
    "title": "Weekly Reflections",
    "section": "What is AGI? How does AGI affect scientific research?",
    "text": "What is AGI? How does AGI affect scientific research?\nI learned that Artificial General Intelligence (AGI) refers to systems capable of performing human level tasks. AGI could help enhance scientific research through faster data analysis, but it also causes ethical concerns in regards to responsible use."
  },
  {
    "objectID": "weekly-reflections.html#week-2-reflection",
    "href": "weekly-reflections.html#week-2-reflection",
    "title": "Weekly Reflections",
    "section": "WEEK 2 REFLECTION",
    "text": "WEEK 2 REFLECTION"
  },
  {
    "objectID": "weekly-reflections.html#how-to-start-using-ai-for-your-project",
    "href": "weekly-reflections.html#how-to-start-using-ai-for-your-project",
    "title": "Weekly Reflections",
    "section": "How to start using AI for your project?",
    "text": "How to start using AI for your project?\nI believe I can use AI in my project by helping me turn our STATA data zip file to help us use it in R studio. AI can help us let the data be read in R studio."
  },
  {
    "objectID": "weekly-reflections.html#which-two-ai-models-are-you-using-how-can-you-leverage-them-to-help-your-research",
    "href": "weekly-reflections.html#which-two-ai-models-are-you-using-how-can-you-leverage-them-to-help-your-research",
    "title": "Weekly Reflections",
    "section": "Which two AI models are you using? How can you leverage them to help your research?",
    "text": "Which two AI models are you using? How can you leverage them to help your research?\nI am using CHATGPT and Google Gemini for my project. I am new to using R, so I will ask it to help me if I run into any errors uploading the STATA zip file to R studio."
  }
]